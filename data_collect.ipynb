{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "# html을 파싱\n",
    "from bs4 import BeautifulSoup\n",
    "# http request를 보내고 http response 를 받기 위해 urllib\n",
    "import urllib.request\n",
    "# regular expression\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from dateutil.parser import parse\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRAWL_DATA():\n",
    "    URL = 'https://www.soccernews.com/soccer-transfers/'\n",
    "    DIR_NAME = './gdrive/My Drive/football_project/csv/'\n",
    "    FILE_NAME = ''\n",
    "    FILE_TYPE = '.csv'\n",
    "    #league_list = ['english-premier-league-transfers', 'spanish-la-liga-transfers', \n",
    "    #              'italian-serie-a-transfers', 'german-bundesliga-transfers', 'rest-of-europe-transfers']\n",
    "    league_list = ['english-premier-league-transfers']\n",
    "    \n",
    "    #for year in range(2018, 2006, -1):\n",
    "    for year in range(2018, 2017, -1):\n",
    "        full_year = '-' + str(year) + '-' + str(year + 1)\n",
    "        print('* crawl data for season ', year, ' ~ ', year + 1)\n",
    "        YEAR = year\n",
    "        for league in league_list:\n",
    "            print('* crawl data for league : ' + league)\n",
    "            FILE_NAME = league\n",
    "            url = URL + league + full_year + '/'\n",
    "            print('* created url : %s' % url)\n",
    "            make_working_directory(DIR_NAME)\n",
    "            res = crawl(url)\n",
    "            res = filter_price(res)\n",
    "            print_list(res)\n",
    "            #transfers = make_transfers(res)\n",
    "            #make_csv(transfers, league)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_working_directory(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        print('* create directory ', dir_name)\n",
    "        os.mkdir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    source_from_url = urllib.request.urlopen(url)\n",
    "    print('* crawl from web ' + url)\n",
    "    # lxml 방식으로 파싱\n",
    "    soup = BeautifulSoup(source_from_url, 'lxml', from_encoding='utf-8')\n",
    "    res = []\n",
    "    # bs4.element.ResultSet, size = 1\n",
    "    for found in soup.find_all('table', limit=1):\n",
    "        tmp = found.find_all(text=True)\n",
    "        filtered = []\n",
    "        arr = []\n",
    "        count = 0\n",
    "        for i in range(len(tmp)):\n",
    "            item = tmp[i]\n",
    "            if item != '\\n':\n",
    "                if item != ' ':\n",
    "                    if item != 'Jan':\n",
    "                        filtered.append(item)\n",
    "        # header 제거\n",
    "        filtered = list(filtered[5:])\n",
    "        \n",
    "        for i in range(len(filtered)):\n",
    "            if count % 6 == 0:\n",
    "                arr = []\n",
    "            item = filtered[i]\n",
    "            if item != '\\n':\n",
    "                if item != ' ':\n",
    "                    arr.append(item.lower())\n",
    "                    count = count + 1\n",
    "            if (count) % 6 == 0:\n",
    "                if len(arr) is not 0:\n",
    "                    res.append(arr)\n",
    "    # print_list(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
